# -*- coding: utf-8 -*-
"""Final segmentation only.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M3u_ZB7K0rMQFJryteUkePNdpfEF3iUD
"""

!pip install datasets farasapy arabert huggingface_hub

import os
import gc
import math
import pickle
import re
import shutil
import json
import warnings
from collections import Counter
import pandas as pd
import psutil
import torch
from torch.utils.data import Dataset as TorchDataset, DataLoader
from datetime import datetime
from tqdm import tqdm
from transformers import (
    AutoModelForMaskedLM,
    AutoTokenizer,
    get_cosine_schedule_with_warmup,
    DataCollatorForLanguageModeling,
    pipeline
)
from accelerate import Accelerator
from datasets import load_dataset, Dataset, DatasetDict
from tokenizers.pre_tokenizers import Whitespace

warnings.filterwarnings("ignore", category=UserWarning, message="Detected pickle protocol 4")
warnings.filterwarnings("ignore", category=FutureWarning, message="You are using `torch.load` with `weights_only=False`")

from huggingface_hub import login
from farasa.segmenter import FarasaSegmenter

login("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")
print("Loading oscar dataset (Arabic) in non-streaming mode...")

dataset = load_dataset("oscar-corpus/oscar", "unshuffled_deduplicated_ar", split="train")

sample_size = 1000

dataset = dataset.shuffle(seed=42).select(range(sample_size))

print(f"Loaded {len(dataset)} samples.")

farasa_segmenter = FarasaSegmenter(interactive=True)

def preprocess_with_farasa(text):
    unwanted_chars = r'[()\[\]:«»“”‘’—_,;!?|/\\]'
    text = re.sub(unwanted_chars, '', text)
    text = re.sub(r'(\-\-|\[\]|\.\.)', '', text)
    return farasa_segmenter.segment(text)

def fix_punctuation_spacing(text):
    text = re.sub(r'\s+([؟،,.!؛:])', r'\1', text)
    text = re.sub(r'([؟،,.!؛:])([^\s])', r'\1 \2', text)
    text = re.sub(r'\s{2,}', ' ', text)
    return text.strip()

def is_arabic_text(text):
    arabic_pattern = re.compile(r'^[\u0600-\u06FF\s.,،؛؟!:\-–—«»“”‘’…(){}\[\]\/ـ]+$')
    return bool(arabic_pattern.match(text))

def split_text_into_chunks(text, window_size):
    words = text.split()
    chunks = []
    for i in range(0, len(words), window_size):
        chunk = " ".join(words[i:i+window_size]).strip()
        if chunk:
            chunks.append(chunk)
    return chunks

def process_text(text, window_size):
    processed_text = preprocess_with_farasa(text)
    processed_text = fix_punctuation_spacing(processed_text)
    words = processed_text.split()
    if len(words) > window_size:
        return split_text_into_chunks(processed_text, window_size)
    else:
        return [processed_text]

print("Processing and segmenting texts...")

WINDOW_SIZE = 8192

processed_texts = []

for example in dataset:
    text = example["text"]
    if is_arabic_text(text):
        chunks = process_text(text, WINDOW_SIZE)
        processed_texts.extend(chunks)

print(f"Total processed chunks: {len(processed_texts)}")
print("Creating HF Dataset and splitting into train/validation/test splits...")

final_dataset = Dataset.from_dict({"text": processed_texts})
split_dataset = final_dataset.train_test_split(test_size=0.02, seed=42)
val_test_split = split_dataset["test"].train_test_split(test_size=0.5, seed=42)
dataset_dict = DatasetDict({
    "train": split_dataset["train"],
    "validation": val_test_split["train"],
    "test": val_test_split["test"]
})

print("Saving splits to TXT files...")

os.makedirs("/content/train", exist_ok=True)
os.makedirs("/content/validation", exist_ok=True)
os.makedirs("/content/test", exist_ok=True)

with open("/content/train/train.txt", "w", encoding="utf-8") as f:
    for example in dataset_dict["train"]:
        f.write(example["text"] + "\n")
with open("/content/validation/validation.txt", "w", encoding="utf-8") as f:
    for example in dataset_dict["validation"]:
        f.write(example["text"] + "\n")
with open("/content/test/test.txt", "w", encoding="utf-8") as f:
    for example in dataset_dict["test"]:
        f.write(example["text"] + "\n")

print("Dataset segmentation and splitting complete.")
print("Files saved: train.txt, validation.txt, test.txt")