# -*- coding: utf-8 -*-
"""Labeling _Arabic_Oscar_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e4_X4X2yyBm4-QuGBHfrKWhJ0kgwzhW8

data
"""

!pip install datasets
!pip install farasapy
!pip install arabert

from huggingface_hub import login
from datasets import load_dataset, Dataset
import re
from farasa.segmenter import FarasaSegmenter
import random
import pandas as pd

login("")
dataset = load_dataset("oscar-corpus/oscar", "unshuffled_deduplicated_ar", split="train", streaming=True)

farasa_segmenter = FarasaSegmenter(interactive=True)

def preprocess_with_farasa(text):
    unwanted_chars = r'[()\[\]:«»“”‘’—_,;!?|/\\]'
    text = re.sub(unwanted_chars, '', text)
    text = re.sub(r'(\-\-|\[\]|\.\.)', '', text)
    return farasa_segmenter.segment(text)

def split_text_into_sentences_with_minimum_length(text, min_words=10):
    words = text.split()
    total_words = len(words)

    if total_words < min_words * 2:
        return text.strip(), ""
    split_point = int(total_words * 0.5)
    if split_point < min_words:
        split_point = min_words
    elif total_words - split_point < min_words:
        split_point = total_words - min_words
    sentence_A = " ".join(words[:split_point]).strip()
    sentence_B = " ".join(words[split_point:]).strip()
    return sentence_A, sentence_B

def is_arabic_text(text):
    arabic_pattern = re.compile(r'^[\u0600-\u06FF\s.,،؛؟!:\-–—«»“”‘’…(){}\[\]\/ـ]+$')
    return bool(arabic_pattern.match(text))

def create_unrelated_pairs(data, num_pairs):
    unrelated_pairs = []
    all_sentences = [row[0] for row in data]
    random.shuffle(all_sentences)
    for _ in range(num_pairs):
        sentence_A = random.choice(all_sentences)
        sentence_B = random.choice(all_sentences)
        if sentence_A != sentence_B and len(set(sentence_A.split()) & set(sentence_B.split())) < 3:
            unrelated_pairs.append((sentence_A, sentence_B, 0))
    return unrelated_pairs

def balance_dataset(dataset, strategy="undersample"):
    class_0 = [row for row in dataset if row[2] == 0]
    class_1 = [row for row in dataset if row[2] == 1]
    if strategy == "undersample":
        if len(class_0) > len(class_1):
            class_0 = random.sample(class_0, len(class_1))
        else:
            class_1 = random.sample(class_1, len(class_0))
    elif strategy == "oversample":
        if len(class_0) > len(class_1):
            class_1 = random.choices(class_1, k=len(class_0))
        else:
            class_0 = random.choices(class_0, k=len(class_1))
    else:
        raise ValueError("Invalid strategy. Choose 'undersample' or 'oversample'.")
    balanced_dataset = class_0 + class_1
    random.shuffle(balanced_dataset)
    return balanced_dataset

def process_batch(batch):
    processed_data = []
    for row in batch["text"]:
        sentence_A, sentence_B = split_text_into_sentences_with_minimum_length(row)
        if sentence_A and sentence_B and is_arabic_text(sentence_A) and is_arabic_text(sentence_B):
            sentence_A = preprocess_with_farasa(sentence_A)
            sentence_B = preprocess_with_farasa(sentence_B)
            processed_data.append((sentence_A, sentence_B, 1))
    return processed_data

BATCH_SIZE = 1000
output_file_path = "cleaned_dataset.csv"

with open(output_file_path, "w") as f:
    f.write("sentence_A,sentence_B,label\n")

    for i, batch in enumerate(dataset.batch(BATCH_SIZE)):
        print(f"Processing batch {i+1}")

        processed_data = process_batch(batch)

        num_pairs = len(processed_data)
        unrelated_pairs = create_unrelated_pairs(processed_data, num_pairs)

        combined_data = processed_data + unrelated_pairs
        balanced_data = balance_dataset(combined_data, strategy="undersample")

        for row in balanced_data:
            f.write(f"{row[0]},{row[1]},{row[2]}\n")

print(f"Dataset processing complete. Saved to {output_file_path}.")

from google.colab import files
files.download(output_file_path)